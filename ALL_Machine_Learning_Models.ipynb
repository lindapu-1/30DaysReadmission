{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOTkUx054XGgkaZAdLk1p+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lindapu-1/30DaysReadmission/blob/main/ALL_Machine_Learning_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YRbgvg1u8U6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "zip_data_path = \"/gdrive/MyDrive/all_data_3612.zip\"\n",
        "\n",
        "!mkdir all_data_3612\n",
        "extract_path = '/content/all_data_3612'\n",
        "\n",
        "with zipfile.ZipFile(zip_data_path, 'r') as zip_ref:\n",
        "   zip_ref.extractall(extract_path)\n",
        "\n",
        "with open('/content/all_data_3612/ehr_preprocessed_seq_by_day_cat_embedding.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Kzm9WLvZRD",
        "outputId": "1b9d7172-4ea9-42e0-cbf6-87b7a4a31b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['feat_dict', 'feature_cols', 'cat_idxs', 'cat_dims', 'demo_cols', 'icd_cols', 'lab_cols', 'med_cols'])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"/content/all_data_3612/train.csv\")\n",
        "df_valid = pd.read_csv(\"/content/all_data_3612/valid.csv\")\n",
        "df_test = pd.read_csv(\"/content/all_data_3612/test.csv\")"
      ],
      "metadata": {
        "id": "ftr1M-z0joNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_labels = pd.read_csv('1.csv')\n",
        "df_test = df_test.merge(df_labels, on='id')"
      ],
      "metadata": {
        "id": "teUZhimPBJF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_train, df_valid])"
      ],
      "metadata": {
        "id": "RG83mlm8Dot1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data prepare"
      ],
      "metadata": {
        "id": "91Hp17dm-Wmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wr8HZb7-ECy"
      },
      "outputs": [],
      "source": [
        "#@title 正常dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, feat_dict, include_labels=True):\n",
        "        self.df = dataframe.drop_duplicates(subset=['id'], keep='first')\n",
        "        self.feat_dict = feat_dict\n",
        "        self.include_labels = include_labels\n",
        "        self.mean, self.std = self.compute_stats()\n",
        "\n",
        "    def compute_stats(self):\n",
        "        # 将所有特征合并到一个大数组中\n",
        "        all_features = torch.cat([torch.tensor(self.feat_dict[id], dtype=torch.float32) for id in self.df['id']])\n",
        "        # 计算均值和标准差\n",
        "        mean = all_features.mean(dim=0)\n",
        "        std = all_features.std(dim=0)\n",
        "        return mean, std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        features = torch.tensor(self.feat_dict[row['id']], dtype=torch.float32)\n",
        "\n",
        "        #features = (features - self.mean) / (self.std + 1e-6)  # 防止除以零\n",
        "        if self.include_labels:\n",
        "            label = row['readmitted_within_30days']\n",
        "            return features, torch.tensor(label, dtype=torch.long)\n",
        "        else:\n",
        "            return features\n",
        "\n",
        "train_dataset = CustomDataset(df, data['feat_dict'])\n",
        "test_dataset=CustomDataset(df_test, data['feat_dict'], include_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def extract_features_and_labels(dataset, feature_indices=None, retain_TS=False):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for features, label in dataset:\n",
        "        # 如果提供了特征索引，只选择这些特征\n",
        "        if feature_indices is not None:\n",
        "            selected_features = features[:, feature_indices]\n",
        "        else:\n",
        "            selected_features = features\n",
        "\n",
        "        # 检查是否保留时间序列\n",
        "        if retain_TS:\n",
        "            # 保留时间序列的第一个维度\n",
        "            if selected_features.shape[0] < 30:\n",
        "                # 如果少于30天，使用最后一天的值进行填充\n",
        "                padding = torch.ones((30 - selected_features.shape[0], *selected_features.shape[1:])) * selected_features[-1]\n",
        "                selected_features = torch.cat([selected_features, padding], dim=0)\n",
        "            else:\n",
        "                # 只取最后30天的值\n",
        "                selected_features = selected_features[-30:]\n",
        "        else:\n",
        "            # 对时间维度进行平均\n",
        "            selected_features = selected_features.mean(dim=0)  # 对时间维度取平均值\n",
        "\n",
        "            # 添加住院天数作为一个新的特征\n",
        "            num_days = features.shape[0]  # 时间维度的大小就是住院天数\n",
        "            selected_features = torch.cat([selected_features, torch.tensor([num_days], dtype=torch.float32)])\n",
        "\n",
        "        features_list.append(selected_features)  # 存储特征\n",
        "        labels_list.append(label)  # 存储标签\n",
        "\n",
        "    # 将列表转换为Tensor\n",
        "    features_tensor = torch.stack(features_list)\n",
        "    labels_tensor = torch.stack(labels_list)\n",
        "\n",
        "    return features_tensor, labels_tensor"
      ],
      "metadata": {
        "id": "xaKBjxvr_lnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title indice\n",
        "demo_indices = list(range(3))  # demo 特征在前3个位置\n",
        "\n",
        "icd_cols_indices = list(range(3, 3+91))  # icd_cols 特征在接下来的91个位置\n",
        "\n",
        "\n",
        "lab_cols_indices = list(range(3+91, 3+91+36))  # lab_cols 特征在接下来的36个位置\n",
        "#加一个time feature\n",
        "\n",
        "med_cols_indices = list(range(3+91+36, 3+91+36+41))  # med_cols 特征在接下来的41个位置\n",
        "#加一个feature为这些value的standarlized sum（？）\n",
        "\n",
        "# 打印结果\n",
        "print(\"demo_indices:\", demo_indices)\n",
        "print(\"icd_cols_indices:\", icd_cols_indices)\n",
        "print(\"lab_cols_indices:\", lab_cols_indices)\n",
        "print(\"med_cols_indices:\", med_cols_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJc-0HN7IiGF",
        "outputId": "7f0ebf83-6926-4262-dc4f-7cf665c1057b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "demo_indices: [0, 1, 2]\n",
            "icd_cols_indices: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93]\n",
            "lab_cols_indices: [94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]\n",
            "med_cols_indices: [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提取训练和测试特征\n",
        "feature_indices_x1=demo_indices+icd_cols_indices+lab_cols_indices+med_cols_indices\n",
        "feature_indices_x2=lab_cols_indices\n",
        "\n",
        "train_features_1, train_labels = extract_features_and_labels(train_dataset,feature_indices_x1, retain_TS=False)\n",
        "train_features_2, train_labels = extract_features_and_labels(train_dataset,feature_indices_x2, retain_TS=True)\n",
        "\n",
        "test_features_1, test_labels = extract_features_and_labels(train_dataset,feature_indices_x1, retain_TS=False)\n",
        "test_features_2, test_labels = extract_features_and_labels(train_dataset,feature_indices_x2, retain_TS=True)#last 30 days\n",
        "#train_features_1, train_features_2->train_labels 训练\n",
        "#test_features_1, test_features_2->test_labels 评估"
      ],
      "metadata": {
        "id": "vX4UhR8IIpcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_2.shape"
      ],
      "metadata": {
        "id": "7k1iX4grTP3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 三个model\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout_rate=0.8):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # 定义三层LSTM，每层之间添加dropout\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout_rate)\n",
        "        # Dropout层\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # 定义线性层\n",
        "        self.fc_token = nn.Linear(hidden_size, output_size)  # 用于 token 改变量的预测\n",
        "        self.fc_label = nn.Linear(hidden_size, 3)  # 用于每个时间步的标签预测\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM 层\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # 应用dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # 在每个时间步预测 token 的改变量\n",
        "        token_change = self.fc_token(lstm_out)\n",
        "        # 在每个时间步预测标签\n",
        "        labels = self.fc_label(lstm_out)\n",
        "        return token_change, labels\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        #out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "class SimpleFusionNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.5):\n",
        "        super(SimpleFusionNetwork, self).__init__()\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid() #因为pred是prob所以框在0-1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "class VerySimpleFusionNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "      super(VerySimpleFusionNetwork, self).__init__()\n",
        "      self.fc = nn.Linear(input_dim, output_dim)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, lstm_input_size, lstm_hidden_size, lstm_output_size, nn_input_dim, nn_output_dim, fusion_input_dim, fusion_hidden_dim, dropout_rate=0.5):\n",
        "        super(FusionModel, self).__init__()\n",
        "\n",
        "        # 定义LSTM模型\n",
        "        self.lstm_model = LSTMModel(lstm_input_size, lstm_hidden_size, lstm_output_size, dropout_rate=dropout_rate)\n",
        "\n",
        "        # 定义逻辑回归模型\n",
        "        self.nn_model = LogisticRegression(nn_input_dim, nn_output_dim)\n",
        "\n",
        "        # 定义融合网络\n",
        "        self.fusion_model = SimpleFusionNetwork(fusion_input_dim, fusion_hidden_dim, dropout_rate=dropout_rate)\n",
        "\n",
        "    def forward(self, batch_features_1, batch_features_2):\n",
        "        _, lstm_out = self.lstm_model(batch_features_2)\n",
        "        nn_out = self.nn_model(batch_features_1)\n",
        "\n",
        "        # 取 LSTM 输出的最后一个时间步\n",
        "        lstm_out_last = lstm_out[:, -1, :]\n",
        "\n",
        "        # 融合 LSTM 和 NN 的输出\n",
        "        fusion_in = torch.cat((nn_out, lstm_out_last), dim=1)\n",
        "        fusion_out = self.fusion_model(fusion_in)\n",
        "\n",
        "        return fusion_out"
      ],
      "metadata": {
        "id": "afs-e6OwPcb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()  # 添加 Sigmoid 激活函数\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)  # 确保输出在 [0,1] 范围内\n",
        "        return out"
      ],
      "metadata": {
        "id": "KpAtNkwJxAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout_rate=0.8):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # 定义三层LSTM，每层之间添加dropout\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout_rate)\n",
        "        # Dropout层\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # 定义线性层\n",
        "        self.fc_token = nn.Linear(hidden_size, output_size)  # 用于 token 改变量的预测\n",
        "        self.fc_label = nn.Linear(hidden_size, 3)  # 用于每个时间步的标签预测\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM 层\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # 应用dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # 在每个时间步预测 token 的改变量\n",
        "        token_change = self.fc_token(lstm_out)\n",
        "        # 在每个时间步预测标签\n",
        "        labels = self.fc_label(lstm_out)\n",
        "        return token_change, labels"
      ],
      "metadata": {
        "id": "1JVTp73RxLgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialze model + dataloader\n",
        "nn_model = LogisticRegression(input_dim=136,output_dim=1)\n",
        "lstm_model = LSTMModel(input_size=36, hidden_size=128, output_size=50)\n",
        "fusion_model = VerySimpleFusionNetwork(input_dim=2, output_dim=1)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_features_1 = torch.tensor(train_features_1, dtype=torch.float32)\n",
        "train_features_2 = torch.tensor(train_features_2, dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
        "\n",
        "test_features_1 = torch.tensor(test_features_1, dtype=torch.float32)\n",
        "test_features_2 = torch.tensor(test_features_2, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.float32)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_features_1, train_features_2, train_labels)\n",
        "test_dataset = TensorDataset(test_features_1, test_features_2, test_labels)\n",
        "\n",
        "#dataloader\n",
        "batch_size = 1000\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOR0oanbRVZK",
        "outputId": "1a060238-1825-48a0-d091-f2f2ee2d59ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-5e9fe672406f>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_features_1 = torch.tensor(train_features_1, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_features_2 = torch.tensor(train_features_2, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.tensor(train_labels, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_features_1 = torch.tensor(test_features_1, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_features_2 = torch.tensor(test_features_2, dtype=torch.float32)\n",
            "<ipython-input-79-5e9fe672406f>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_labels = torch.tensor(test_labels, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_indices = list(range(3))  # demo 特征在前3个位置\n",
        "\n",
        "icd_cols_indices = list(range(3, 3+91))  # icd_cols 特征在接下来的91个位置\n",
        "\n",
        "\n",
        "lab_cols_indices = list(range(3+91, 3+91+36))  # lab_cols 特征在接下来的36个位置\n",
        "#加一个time feature\n",
        "\n",
        "med_cols_indices = list(range(3+91+36, 3+91+36+41))\n",
        "\n",
        "\n",
        "print(\n",
        "    data['feat_dict']['11674366_29673314'][1][3:3+91]\n",
        ")\n",
        "print(data['feat_dict']['11674366_29673314'][1][3:3+91])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyorhozBsJ6i",
        "outputId": "fd0a4ae6-6131-4759-b514-8523e3db183a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title all model train and evaluate:\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs=5\n",
        "learning_rate=1e-4\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = optim.SGD(list(nn_model.parameters()) + list(lstm_model.parameters()) + list(fusion_model.parameters()), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_features_1, batch_features_2, batch_labels in train_dataloader:\n",
        "\n",
        "        nn_out = nn_model(batch_features_1)\n",
        "        _,lstm_out = lstm_model(batch_features_2)\n",
        "\n",
        "        # 取最后一个时间步的输出\n",
        "        lstm_out_last = lstm_out[:, -1, :]\n",
        "\n",
        "        fusion_in = torch.cat((nn_out, lstm_out_last), dim=1)\n",
        "        fusion_out = fusion_model(fusion_in)\n",
        "\n",
        "        # 计算loss\n",
        "        batch_labels = batch_labels.unsqueeze(1)\n",
        "        loss = criterion(fusion_out, batch_labels)\n",
        "\n",
        "        # 反向传播和优化\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f'Loss: {loss.item()}')\n",
        "\n",
        "    #一个epoch结束\n",
        "    nn_model.eval()\n",
        "    lstm_model.eval()\n",
        "    fusion_model.eval()\n",
        "\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "\n",
        "    #infer+evaluate\n",
        "    with torch.no_grad():\n",
        "        for batch_features_1, batch_features_2, batch_labels in test_dataloader:\n",
        "            nn_out = nn_model(batch_features_1)\n",
        "            _, lstm_out = lstm_model(batch_features_2)\n",
        "            lstm_out_last = lstm_out[:, -1, :]\n",
        "            fusion_in = torch.cat((nn_out, lstm_out_last), dim=1)\n",
        "            fusion_out = fusion_model(fusion_in)\n",
        "\n",
        "            probas = fusion_out.squeeze().cpu().numpy()\n",
        "            preds.extend(probas)\n",
        "            true_labels.extend(batch_labels.squeeze().cpu().numpy())\n",
        "\n",
        "    # 计算本次epoch的AUC\n",
        "    auc = roc_auc_score(true_labels, preds)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test AUC: {auc}')\n",
        "\n",
        "    #训练模式\n",
        "    nn_model.train()\n",
        "    lstm_model.train()\n",
        "    fusion_model.train()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZhpxG7mbUVto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train and evaluate: part 1\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.optim as optim\n",
        "\n",
        "input_dim = 172  # 这是一个示例值，你需要根据你的数据来设置\n",
        "hidden_dim = 10  # 这也是一个示例值，你可以根据你的需求来设置\n",
        "output_dim = 1  # 这是一个例子，假设你的问题是二分类问题\n",
        "\n",
        "nn_model = LogisticRegression(input_dim, output_dim)\n",
        "\n",
        "num_epochs=10\n",
        "learning_rate=1e-4\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = optim.SGD(list(nn_model.parameters()), lr=learning_rate)  # 只优化 nn_model 的参数\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_features_1, _, batch_labels in test_dataloader:  # 只使用 feature1 和 labels\n",
        "\n",
        "        nn_out = nn_model(batch_features_1)\n",
        "\n",
        "        # 计算loss\n",
        "        batch_labels = batch_labels.unsqueeze(1)\n",
        "        loss = criterion(nn_out, batch_labels)  # 直接使用 nn_out 计算 loss\n",
        "\n",
        "        # 反向传播和优化\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f'Loss: {loss.item()}')\n",
        "\n",
        "    #一个epoch结束\n",
        "    nn_model.eval()\n",
        "\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "\n",
        "    #infer+evaluate\n",
        "    with torch.no_grad():\n",
        "        for batch_features_1, _,batch_labels in test_dataloader:  # 只使用 feature1 和 labels\n",
        "            nn_out = nn_model(batch_features_1)\n",
        "\n",
        "            probas = nn_out.squeeze().cpu().numpy()\n",
        "            preds.extend(probas)\n",
        "            true_labels.extend(batch_labels.squeeze().cpu().numpy())\n",
        "\n",
        "    # 计算本次epoch的AUC\n",
        "    auc = roc_auc_score(true_labels, preds)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Test AUC: {auc}')\n",
        "\n",
        "    #训练模式\n",
        "    nn_model.train()"
      ],
      "metadata": {
        "id": "rdOCTfXzxl38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化模型\n",
        "input_dim = 100  # 例如，你的输入维度是100\n",
        "output_dim = 1  # 例如，你的输出维度是1\n",
        "model = LogisticRegression(input_dim, output_dim)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.BCEWithLogitsLoss()  # 由于你没有在模型中使用sigmoid，所以这里使用BCEWithLogitsLoss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # 使用SGD优化器\n",
        "\n",
        "# 训练模型\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_features, batch_labels in train_dataloader:\n",
        "        # 前向传播\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 打印损失信息\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "vSncfCoLpJwX",
        "outputId": "d26ed1a0-ce7e-4bb2-97de-f46cdc80298c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-f964e2456959>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# 前向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_7rJrx3Xypl",
        "outputId": "dcb10c50-4d3f-4d2b-921a-9477920b6e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}